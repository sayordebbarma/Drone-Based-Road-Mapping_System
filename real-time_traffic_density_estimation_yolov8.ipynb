{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:28.605394Z",
     "iopub.status.busy": "2024-01-07T16:41:28.605102Z",
     "iopub.status.idle": "2024-01-07T16:41:33.110352Z",
     "shell.execute_reply": "2024-01-07T16:41:33.109382Z",
     "shell.execute_reply.started": "2024-01-07T16:41:28.605368Z"
    },
    "papermill": {
     "duration": 5.616125,
     "end_time": "2023-12-12T12:45:08.661490",
     "exception": false,
     "start_time": "2023-12-12T12:45:03.045365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disable warnings in the notebook to maintain clean output cells\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:33.111939Z",
     "iopub.status.busy": "2024-01-07T16:41:33.111517Z",
     "iopub.status.idle": "2024-01-07T16:41:33.116783Z",
     "shell.execute_reply": "2024-01-07T16:41:33.115938Z",
     "shell.execute_reply.started": "2024-01-07T16:41:33.111912Z"
    },
    "papermill": {
     "duration": 0.027946,
     "end_time": "2023-12-12T12:45:08.709398",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.681452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the visual appearance of Seaborn plots\n",
    "sns.set(rc={'axes.facecolor': '#eae8fa'}, style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:33.118523Z",
     "iopub.status.busy": "2024-01-07T16:41:33.118193Z",
     "iopub.status.idle": "2024-01-07T16:41:34.004717Z",
     "shell.execute_reply": "2024-01-07T16:41:34.003790Z",
     "shell.execute_reply.started": "2024-01-07T16:41:33.118492Z"
    },
    "papermill": {
     "duration": 0.536317,
     "end_time": "2023-12-12T12:45:09.490231",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.953914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a pretrained YOLOv8n model from Ultralytics\n",
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:34.008052Z",
     "iopub.status.busy": "2024-01-07T16:41:34.007757Z",
     "iopub.status.idle": "2024-01-07T16:41:36.773138Z",
     "shell.execute_reply": "2024-01-07T16:41:36.771864Z",
     "shell.execute_reply.started": "2024-01-07T16:41:34.008028Z"
    },
    "papermill": {
     "duration": 8.967725,
     "end_time": "2023-12-12T12:45:18.515569",
     "exception": false,
     "start_time": "2023-12-12T12:45:09.547844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the image file\n",
    "image_path = 'Vehicle_Detection_Image_Dataset/sample_image2.jpg'\n",
    "\n",
    "# Perform inference on the provided image(s)\n",
    "results = model.predict(source=image_path, \n",
    "                        imgsz=640,  # Resize image to 640x640 (the size pf images the model was trained on)\n",
    "                        conf=0.5)   # Confidence threshold: 50% (only detections above 50% confidence will be considered)\n",
    "\n",
    "# Annotate and convert image to numpy array\n",
    "sample_image = results[0].plot(line_width=2)\n",
    "\n",
    "# Convert the color of the image from BGR to RGB for correct color representation in matplotlib\n",
    "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display annotated image\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Detected Vehicles in Sample Image by the Pre-trained YOLOv8 Model', fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:36.774876Z",
     "iopub.status.busy": "2024-01-07T16:41:36.774521Z",
     "iopub.status.idle": "2024-01-07T16:41:36.788027Z",
     "shell.execute_reply": "2024-01-07T16:41:36.787189Z",
     "shell.execute_reply.started": "2024-01-07T16:41:36.774845Z"
    },
    "papermill": {
     "duration": 0.066253,
     "end_time": "2023-12-12T12:45:19.099731",
     "exception": false,
     "start_time": "2023-12-12T12:45:19.033478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dataset_path\n",
    "dataset_path = 'Vehicle_Detection_Image_Dataset'\n",
    "\n",
    "# Set the path to the YAML file\n",
    "yaml_file_path = os.path.join(dataset_path, 'data.yaml')\n",
    "\n",
    "# Load and print the contents of the YAML file\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_content = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    print(yaml.dump(yaml_content, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:36.789658Z",
     "iopub.status.busy": "2024-01-07T16:41:36.789244Z",
     "iopub.status.idle": "2024-01-07T16:41:39.110870Z",
     "shell.execute_reply": "2024-01-07T16:41:39.109926Z",
     "shell.execute_reply.started": "2024-01-07T16:41:36.789629Z"
    },
    "papermill": {
     "duration": 4.275063,
     "end_time": "2023-12-12T12:45:23.611095",
     "exception": false,
     "start_time": "2023-12-12T12:45:19.336032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set paths for training and validation image sets\n",
    "train_images_path = os.path.join(dataset_path, 'train', 'images')\n",
    "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
    "\n",
    "# Initialize counters for the number of images\n",
    "num_train_images = 0\n",
    "num_valid_images = 0\n",
    "\n",
    "# Initialize sets to hold the unique sizes of images\n",
    "train_image_sizes = set()\n",
    "valid_image_sizes = set()\n",
    "\n",
    "# Check train images sizes and count\n",
    "for filename in os.listdir(train_images_path):\n",
    "    if filename.endswith('.jpg'):  \n",
    "        num_train_images += 1\n",
    "        image_path = os.path.join(train_images_path, filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            train_image_sizes.add(img.size)\n",
    "\n",
    "# Check validation images sizes and count\n",
    "for filename in os.listdir(valid_images_path):\n",
    "    if filename.endswith('.jpg'): \n",
    "        num_valid_images += 1\n",
    "        image_path = os.path.join(valid_images_path, filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            valid_image_sizes.add(img.size)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of training images: {num_train_images}\")\n",
    "print(f\"Number of validation images: {num_valid_images}\")\n",
    "\n",
    "# Check if all images in training set have the same size\n",
    "if len(train_image_sizes) == 1:\n",
    "    print(f\"All training images have the same size: {train_image_sizes.pop()}\")\n",
    "else:\n",
    "    print(\"Training images have varying sizes.\")\n",
    "\n",
    "# Check if all images in validation set have the same size\n",
    "if len(valid_image_sizes) == 1:\n",
    "    print(f\"All validation images have the same size: {valid_image_sizes.pop()}\")\n",
    "else:\n",
    "    print(\"Validation images have varying sizes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:39.112348Z",
     "iopub.status.busy": "2024-01-07T16:41:39.112057Z",
     "iopub.status.idle": "2024-01-07T16:41:41.729949Z",
     "shell.execute_reply": "2024-01-07T16:41:41.728821Z",
     "shell.execute_reply.started": "2024-01-07T16:41:39.112322Z"
    },
    "papermill": {
     "duration": 2.758546,
     "end_time": "2023-12-12T12:45:26.614041",
     "exception": false,
     "start_time": "2023-12-12T12:45:23.855495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all jpg images in the directory\n",
    "image_files = [file for file in os.listdir(train_images_path) if file.endswith('.jpg')]\n",
    "\n",
    "# Select 8 images at equal intervals\n",
    "num_images = len(image_files)\n",
    "selected_images = [image_files[i] for i in range(0, num_images, num_images // 8)]\n",
    "\n",
    "# Create a 2x4 subplot\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 11))\n",
    "\n",
    "# Display each of the selected images\n",
    "for ax, img_file in zip(axes.ravel(), selected_images):\n",
    "    img_path = os.path.join(train_images_path, img_file)\n",
    "    image = Image.open(img_path)\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')  \n",
    "\n",
    "plt.suptitle('Sample Images from Training Dataset', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:41.731615Z",
     "iopub.status.busy": "2024-01-07T16:41:41.731294Z",
     "iopub.status.idle": "2024-01-07T16:55:44.327808Z",
     "shell.execute_reply": "2024-01-07T16:55:44.326530Z",
     "shell.execute_reply.started": "2024-01-07T16:41:41.731585Z"
    },
    "papermill": {
     "duration": 23.627724,
     "end_time": "2023-12-12T12:45:50.728563",
     "exception": true,
     "start_time": "2023-12-12T12:45:27.100839",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model on our custom dataset\n",
    "results = model.train(\n",
    "    data=yaml_file_path,     # Path to the dataset configuration file\n",
    "    epochs=100,              # Number of epochs to train for\n",
    "    imgsz=640,               # Size of input images as integer\n",
    "    device='cpu',            # Device to run on, i.e. cuda device=0 \n",
    "    patience=50,             # Epochs to wait for no observable improvement for early stopping of training\n",
    "    batch=32,                # Number of images per batch\n",
    "    optimizer='auto',        # Optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]\n",
    "    lr0=0.0001,              # Initial learning rate \n",
    "    lrf=0.1,                 # Final learning rate (lr0 * lrf)\n",
    "    dropout=0.1,             # Use dropout regularization\n",
    "    seed=0                   # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:44.330680Z",
     "iopub.status.busy": "2024-01-07T16:55:44.330087Z",
     "iopub.status.idle": "2024-01-07T16:55:45.379173Z",
     "shell.execute_reply": "2024-01-07T16:55:45.377605Z",
     "shell.execute_reply.started": "2024-01-07T16:55:44.330643Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Define the path to the directory\n",
    "post_training_files_path = 'runs/detect/train'\n",
    "\n",
    "# List the files in the directory\n",
    "!dir \"{post_training_files_path}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:45.381771Z",
     "iopub.status.busy": "2024-01-07T16:55:45.381361Z",
     "iopub.status.idle": "2024-01-07T16:55:45.391793Z",
     "shell.execute_reply": "2024-01-07T16:55:45.390676Z",
     "shell.execute_reply.started": "2024-01-07T16:55:45.381734Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to plot learning curves for loss values\n",
    "def plot_learning_curve(df, train_loss_col, val_loss_col, title):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.lineplot(data=df, x='epoch', y=train_loss_col, label='Train Loss', color='#141140', linestyle='-', linewidth=2)\n",
    "    sns.lineplot(data=df, x='epoch', y=val_loss_col, label='Validation Loss', color='orangered', linestyle='--', linewidth=2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:45.393767Z",
     "iopub.status.busy": "2024-01-07T16:55:45.393389Z",
     "iopub.status.idle": "2024-01-07T16:55:46.863097Z",
     "shell.execute_reply": "2024-01-07T16:55:46.862056Z",
     "shell.execute_reply.started": "2024-01-07T16:55:45.393731Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the full file path for 'results.csv' using the directory path and file name\n",
    "results_csv_path = os.path.join(post_training_files_path, 'results.csv')\n",
    "\n",
    "# Load the CSV file from the constructed path into a pandas DataFrame\n",
    "df = pd.read_csv(results_csv_path)\n",
    "\n",
    "# Remove any leading whitespace from the column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Plot the learning curves for each loss\n",
    "plot_learning_curve(df, 'train/box_loss', 'val/box_loss', 'Box Loss Learning Curve')\n",
    "plot_learning_curve(df, 'train/cls_loss', 'val/cls_loss', 'Classification Loss Learning Curve')\n",
    "plot_learning_curve(df, 'train/dfl_loss', 'val/dfl_loss', 'Distribution Focal Loss Learning Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:46.865642Z",
     "iopub.status.busy": "2024-01-07T16:55:46.864744Z",
     "iopub.status.idle": "2024-01-07T16:55:48.072583Z",
     "shell.execute_reply": "2024-01-07T16:55:48.070774Z",
     "shell.execute_reply.started": "2024-01-07T16:55:46.865601Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct the path to the normalized confusion matrix image\n",
    "confusion_matrix_path = os.path.join(post_training_files_path, 'confusion_matrix_normalized.png')\n",
    "\n",
    "# Read the image using cv2\n",
    "cm_img = cv2.imread(confusion_matrix_path)\n",
    "\n",
    "# Convert the image from BGR to RGB color space for accurate color representation with matplotlib\n",
    "cm_img = cv2.cvtColor(cm_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 10), dpi=120)\n",
    "plt.imshow(cm_img)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:48.078246Z",
     "iopub.status.busy": "2024-01-07T16:55:48.077923Z",
     "iopub.status.idle": "2024-01-07T16:56:00.887425Z",
     "shell.execute_reply": "2024-01-07T16:56:00.886243Z",
     "shell.execute_reply.started": "2024-01-07T16:55:48.078220Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct the path to the best model weights file using os.path.join\n",
    "tuned_model_path = os.path.join(post_training_files_path, 'weights/best.pt')\n",
    "\n",
    "# Load the best model weights into the YOLO model\n",
    "tuned_model = YOLO(tuned_model_path)\n",
    "\n",
    "# Validate the best model using the validation set with default parameters\n",
    "metrics = tuned_model.val(split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:00.890082Z",
     "iopub.status.busy": "2024-01-07T16:56:00.889660Z",
     "iopub.status.idle": "2024-01-07T16:56:00.909676Z",
     "shell.execute_reply": "2024-01-07T16:56:00.908441Z",
     "shell.execute_reply.started": "2024-01-07T16:56:00.890045Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the dictionary to a pandas DataFrame and use the keys as the index\n",
    "metrics_df = pd.DataFrame.from_dict(metrics.results_dict, orient='index', columns=['Metric Value'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:00.911548Z",
     "iopub.status.busy": "2024-01-07T16:56:00.911155Z",
     "iopub.status.idle": "2024-01-07T16:56:04.959260Z",
     "shell.execute_reply": "2024-01-07T16:56:04.957846Z",
     "shell.execute_reply.started": "2024-01-07T16:56:00.911511Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the validation images\n",
    "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
    "\n",
    "# List all jpg images in the directory\n",
    "image_files = [file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]\n",
    "\n",
    "# Select 9 images at equal intervals\n",
    "num_images = len(image_files)\n",
    "selected_images = [image_files[i] for i in range(0, num_images, num_images // 9)]\n",
    "\n",
    "# Initialize the subplot\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 21))\n",
    "fig.suptitle('Validation Set Inferences', fontsize=24)\n",
    "\n",
    "# Perform inference on each selected image and display it\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    image_path = os.path.join(valid_images_path, selected_images[i])\n",
    "    results = tuned_model.predict(source=image_path, imgsz=640, conf=0.5)\n",
    "    annotated_image = results[0].plot(line_width=1)\n",
    "    annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(annotated_image_rgb)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:04.961504Z",
     "iopub.status.busy": "2024-01-07T16:56:04.961040Z",
     "iopub.status.idle": "2024-01-07T16:56:06.324053Z",
     "shell.execute_reply": "2024-01-07T16:56:06.322966Z",
     "shell.execute_reply.started": "2024-01-07T16:56:04.961454Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the image file\n",
    "sample_image_path = 'Vehicle_Detection_Image_Dataset/sample_image2.jpg'\n",
    "\n",
    "# Perform inference on the provided image using best model\n",
    "results = tuned_model.predict(source=sample_image_path, imgsz=640, conf=0.1) \n",
    "                        \n",
    "# Annotate and convert image to numpy array\n",
    "sample_image = results[0].plot(line_width=2)\n",
    "\n",
    "# Convert the color of the image from BGR to RGB for correct color representation in matplotlib\n",
    "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display annotated image\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Detected Objects in Sample Image by the tuned YOLOv8 Model', fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:06.325982Z",
     "iopub.status.busy": "2024-01-07T16:56:06.325408Z",
     "iopub.status.idle": "2024-01-07T16:56:25.874908Z",
     "shell.execute_reply": "2024-01-07T16:56:25.873977Z",
     "shell.execute_reply.started": "2024-01-07T16:56:06.325950Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the sample video in the dataset\n",
    "dataset_video_path = 'Vehicle_Detection_Image_Dataset/sample_video.mp4'\n",
    "\n",
    "# Initiate vehicle detection on the sample video using the best performing model and save the output\n",
    "tuned_model.predict(source=dataset_video_path, save=True, conf = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:25.876607Z",
     "iopub.status.busy": "2024-01-07T16:56:25.876267Z",
     "iopub.status.idle": "2024-01-07T16:56:40.862642Z",
     "shell.execute_reply": "2024-01-07T16:56:40.860731Z",
     "shell.execute_reply.started": "2024-01-07T16:56:25.876578Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the .avi video generated by the YOLOv8 prediction to .mp4 format for compatibility with notebook display\n",
    "!ffmpeg -y -loglevel panic -i runs/detect/predict/sample_video.avi processed_sample_video.mp4\n",
    "\n",
    "# Embed and display the processed sample video within the notebook\n",
    "Video(\"processed_sample_video.mp4\", embed=True, width=960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:40.865451Z",
     "iopub.status.busy": "2024-01-07T16:56:40.865105Z",
     "iopub.status.idle": "2024-01-07T16:56:55.842674Z",
     "shell.execute_reply": "2024-01-07T16:56:55.841793Z",
     "shell.execute_reply.started": "2024-01-07T16:56:40.865421Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the threshold for considering traffic as heavy\n",
    "heavy_traffic_threshold = 7\n",
    "moderate_traffic_threshold = 3\n",
    "\n",
    "\n",
    "# Define the vertices for the quadrilaterals\n",
    "vertices1 = np.array([(400, 150), (620, 150), (350, 630), (-900, 630)], dtype=np.int32)\n",
    "vertices2 = np.array([(678, 150), (1000, 150), (2300, 630), (950, 630)], dtype=np.int32)\n",
    "\n",
    "# Define the vertical range for the slice and lane threshold\n",
    "x1, x2 = 145, 635\n",
    "lane_threshold = 620\n",
    "\n",
    "# Define the positions for the text annotations on the image\n",
    "text_position_left_lane = (10, 50)\n",
    "text_position_right_lane = (820, 50)\n",
    "intensity_position_left_lane = (10, 100)\n",
    "intensity_position_right_lane = (820, 100)\n",
    "\n",
    "# Define font, scale, and colors for the annotations\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_color = (0, 255, 0)    # White color for text\n",
    "background_color = (0, 0, 0)  # Red background for text\n",
    "        \n",
    "# Open the video\n",
    "cap = cv2.VideoCapture('Vehicle_Detection_Image_Dataset/sample_video.mp4')\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('traffic_density_analysis1.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "# Read until video is completed\n",
    "while cap.isOpened():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # Create a copy of the original frame to modify\n",
    "        detection_frame = frame.copy()\n",
    "    \n",
    "        # Black out the regions outside the specified vertical range\n",
    "        detection_frame[:x1, :] = 0  # Black out from top to x1\n",
    "        detection_frame[x2:, :] = 0  # Black out from x2 to the bottom of the frame\n",
    "        \n",
    "        # Perform inference on the modified frame\n",
    "        results = tuned_model.predict(detection_frame, imgsz=640, conf=0.2)\n",
    "        processed_frame = results[0].plot(line_width=1)\n",
    "        \n",
    "        # Restore the original top and bottom parts of the frame\n",
    "        processed_frame[:x1, :] = frame[:x1, :].copy()\n",
    "        processed_frame[x2:, :] = frame[x2:, :].copy()        \n",
    "        \n",
    "        # Draw the quadrilaterals on the processed frame\n",
    "        cv2.polylines(processed_frame, [vertices1], isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "        cv2.polylines(processed_frame, [vertices2], isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "        \n",
    "        # Retrieve the bounding boxes from the results\n",
    "        bounding_boxes = results[0].boxes\n",
    "\n",
    "        # Initialize counters for vehicles in each lane\n",
    "        vehicles_in_left_lane = 0\n",
    "        vehicles_in_right_lane = 0\n",
    "\n",
    "        # Loop through each bounding box to count vehicles in each lane\n",
    "        for box in bounding_boxes.xyxy:\n",
    "            # Check if the vehicle is in the left lane based on the x-coordinate of the bounding box\n",
    "            if box[0] < lane_threshold:\n",
    "                vehicles_in_left_lane += 1\n",
    "            else:\n",
    "                vehicles_in_right_lane += 1\n",
    "                \n",
    "        # Determine the traffic intensity for the left lane\n",
    "        traffic_intensity_left = (         \n",
    "            \"Heavy\" if vehicles_in_left_lane > heavy_traffic_threshold \n",
    "            else \"Moderate\" if vehicles_in_left_lane >= moderate_traffic_threshold \n",
    "            else \"Low\"\n",
    "            )\n",
    "\n",
    "        # Determine the traffic intensity for the right lane\n",
    "        traffic_intensity_right = (\n",
    "            \"Heavy\" if vehicles_in_right_lane > heavy_traffic_threshold \n",
    "            else \"Moderate\" if vehicles_in_right_lane >= moderate_traffic_threshold \n",
    "            else \"Low\"\n",
    "            )\n",
    "\n",
    "\n",
    "        # Add a background rectangle for the left lane vehicle count\n",
    "        cv2.rectangle(processed_frame, (text_position_left_lane[0]-10, text_position_left_lane[1] - 25), \n",
    "                      (text_position_left_lane[0] + 460, text_position_left_lane[1] + 10), background_color, -1)\n",
    "\n",
    "        # Add the vehicle count text on top of the rectangle for the left lane\n",
    "        cv2.putText(processed_frame, f'Vehicles in Left Lane: {vehicles_in_left_lane}', text_position_left_lane, \n",
    "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Add a background rectangle for the left lane traffic intensity\n",
    "        cv2.rectangle(processed_frame, (intensity_position_left_lane[0]-10, intensity_position_left_lane[1] - 25), \n",
    "                      (intensity_position_left_lane[0] + 460, intensity_position_left_lane[1] + 10), background_color, -1)\n",
    "\n",
    "        # Add the traffic intensity text on top of the rectangle for the left lane\n",
    "        cv2.putText(processed_frame, f'Traffic Intensity: {traffic_intensity_left}', intensity_position_left_lane, \n",
    "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Add a background rectangle for the right lane vehicle count\n",
    "        cv2.rectangle(processed_frame, (text_position_right_lane[0]-10, text_position_right_lane[1] - 25), \n",
    "                      (text_position_right_lane[0] + 460, text_position_right_lane[1] + 10), background_color, -1)\n",
    "\n",
    "        # Add the vehicle count text on top of the rectangle for the right lane\n",
    "        cv2.putText(processed_frame, f'Vehicles in Right Lane: {vehicles_in_right_lane}', text_position_right_lane, \n",
    "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Add a background rectangle for the right lane traffic intensity\n",
    "        cv2.rectangle(processed_frame, (intensity_position_right_lane[0]-10, intensity_position_right_lane[1] - 25), \n",
    "                      (intensity_position_right_lane[0] + 460, intensity_position_right_lane[1] + 10), background_color, -1)\n",
    "\n",
    "        # Add the traffic intensity text on top of the rectangle for the right lane\n",
    "        cv2.putText(processed_frame, f'Traffic Intensity: {traffic_intensity_right}', intensity_position_right_lane, \n",
    "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        out.write(processed_frame)\n",
    "\n",
    "        cv2.line(processed_frame, (lane_threshold, 0), (lane_threshold, processed_frame.shape[0]), (0, 0, 255), 2)\n",
    "        \n",
    "        # Uncomment the following 3 lines if running this code on a local machine to view the real-time processing results\n",
    "        cv2.imshow('Real-time Analysis', processed_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press Q on keyboard to exit the loop\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video capture and video write objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Close all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:55.844457Z",
     "iopub.status.busy": "2024-01-07T16:56:55.844123Z",
     "iopub.status.idle": "2024-01-07T16:57:08.599235Z",
     "shell.execute_reply": "2024-01-07T16:57:08.597866Z",
     "shell.execute_reply.started": "2024-01-07T16:56:55.844428Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the .avi video generated by our traffic density estimation app to .mp4 format for compatibility with notebook display\n",
    "!ffmpeg -y -loglevel panic -i traffic_density_analysis.avi traffic_density_analysis.mp4\n",
    "\n",
    "# Embed and display the processed sample video within the notebook\n",
    "Video(\"traffic_density_analysis.mp4\", embed=True, width=960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:57:08.601218Z",
     "iopub.status.busy": "2024-01-07T16:57:08.600853Z",
     "iopub.status.idle": "2024-01-07T16:57:10.881191Z",
     "shell.execute_reply": "2024-01-07T16:57:10.880392Z",
     "shell.execute_reply.started": "2024-01-07T16:57:08.601188Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export the model\n",
    "tuned_model.export(format='onnx')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4107330,
     "sourceId": 7166601,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30616,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 68.691048,
   "end_time": "2023-12-12T12:45:53.827760",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-12T12:44:45.136712",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
